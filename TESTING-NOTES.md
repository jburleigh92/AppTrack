# AppTrack Testing Notes

## Test Environment
- **Platform**: GitHub Codespaces (Ubuntu)
- **Database**: PostgreSQL 16
- **Python**: 3.12.1
- **Framework**: FastAPI with Uvicorn

## Phase 1-4: Core Backend Testing âœ… COMPLETE

### Repository & Deployment
- âœ… Repository structure organized (backend/, docs/)
- âœ… GitHub integration and version control
- âœ… Codespaces deployment successful

### Database
- âœ… PostgreSQL 16 installed and configured
- âœ… All Alembic migrations applied successfully
- âœ… 12 tables created with proper relationships
- âœ… Connection pooling and session management working

### API Server
- âœ… FastAPI server running on port 8000
- âœ… Swagger UI accessible at /docs
- âœ… CORS middleware configured
- âœ… All route modules loaded successfully

### Health Checks
- âœ… GET /api/v1/health/health/live â†’ 200 OK
- âœ… GET /api/v1/health/health/ready â†’ 200 OK with database connection verified

### Application Ingestion
- âœ… POST /api/v1/applications/capture (browser source)
  - Created test application: Test Corp - Software Engineer
  - Status: applied, Source: browser
- âœ… POST /api/v1/emails/ingest (email source)
  - Created test application: Acme Corp - Senior Developer
  - Status: applied, Source: email
  - Correlation strategy: created_new

### Timeline Events
- âœ… GET /api/v1/timeline/{application_id}/timeline
  - Returns chronological event list
  - Tracks application_created events
  - 6 total timeline events recorded

### Export Functionality
- âœ… POST /api/v1/exports/csv
  - Successfully generated CSV with 2 applications
  - Includes all fields: company, title, status, dates, notes
  - Proper formatting and headers

## Phase 5: Scraper Service Testing ðŸ”„ IN PROGRESS

### Scraper Queue System
- âœ… POST /api/v1/scraper/scrape endpoint functional
- âœ… Jobs enqueued successfully to scraper_queue table
- âœ… Queue records include: id, application_id, url, status, timestamps

### Background Worker
- âœ… Scraper worker implemented with queue polling
- âœ… Job status transitions: pending â†’ processing â†’ completed/failed
- âœ… Worker processes jobs asynchronously
- âœ… Error handling for HTTP errors (404, timeouts, etc.)
- âœ… Database updates on completion/failure

### Test Results
- **Test 1**: Lever.co test URL (non-existent)
  - Status: failed (404)
  - Error message: "URL not found (404)"
  - Completed at: 2025-12-12 21:58:16
- **Test 2**: Anthropic jobs page
  - Status: failed (404)
  - Worker successfully detected and logged error

### Known Issues
- Test URLs return 404 (expected for non-existent pages)
- Need to test with actual live job posting URLs
- Worker uses deprecated datetime.utcnow() (warning logged)

## Test Data Summary

### Applications Table (2 records)
| ID | Company | Title | Source | Date |
|----|---------|-------|--------|------|
| 5fa2837d... | Test Corp | Software Engineer | browser | 2025-12-12 |
| 573e7096... | Acme Corp | Senior Developer | email | 2025-12-10 |

### Scraper Queue (2 jobs)
| ID | Status | URL | Error |
|----|--------|-----|-------|
| be6e08e1... | failed | lever.co/example/... | URL not found (404) |
| 8a37ab58... | failed | lever.co/anthropic | URL not found (404) |

### Timeline Events (6 events)
- 2x application_created (browser)
- 2x application_created (email)
- 2x scrape_started
- (scrape_failed events expected but may be recorded)

## Outstanding Work

### Phase 5 (Scraper) - Remaining
- [ ] Test with real, live job posting URLs
- [ ] Verify HTML extraction logic
- [ ] Test ATS platform detection (Greenhouse, Lever, Workday, etc.)
- [ ] Validate job_postings table population
- [ ] Test application-to-posting linking

### Phase 6 (AI Analysis)
- [ ] Implement analysis worker
- [ ] Test LLM integration (OpenAI/Anthropic)
- [ ] Verify resume-job matching logic
- [ ] Test analysis_results table population

### Phase 7 (Additional Features)
- [ ] Google Sheets sync functionality
- [ ] End-to-end workflow testing
- [ ] Performance testing with multiple concurrent jobs
- [ ] Error recovery and retry logic

## Commands for Testing

### Start API Server
```bash
cd /workspaces/AppTrack/backend
source venv/bin/activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Start Scraper Worker
```bash
cd /workspaces/AppTrack/backend
source venv/bin/activate
python -m app.workers.scraper_worker
```

### Database Queries
```bash
# Check applications
PGPASSWORD=postgres psql -h localhost -U postgres -d jobtracker -c "SELECT * FROM applications;"

# Check scraper queue
PGPASSWORD=postgres psql -h localhost -U postgres -d jobtracker -c "SELECT * FROM scraper_queue;"

# Check timeline events
PGPASSWORD=postgres psql -h localhost -U postgres -d jobtracker -c "SELECT * FROM timeline_events ORDER BY created_at;"
```

### API Test Commands
```bash
# Health check
curl http://localhost:8000/api/v1/health/health/ready

# Create application
curl -X POST http://localhost:8000/api/v1/applications/capture \
  -H "Content-Type: application/json" \
  -d '{"company_name": "Example Corp", "job_title": "Engineer", "job_posting_url": "https://example.com/job"}'

# Trigger scrape
curl -X POST http://localhost:8000/api/v1/scraper/scrape \
  -H "Content-Type: application/json" \
  -d '{"application_id": "UUID", "url": "https://jobs.lever.co/..."}'

# Export CSV
curl -X POST http://localhost:8000/api/v1/exports/csv \
  -H "Content-Type: application/json" \
  -d '{}' -o export.csv
```

## Notes
- All timestamps are in UTC
- Database connection uses connection pooling (pool_size=10, max_overflow=20)
- API server auto-reloads on code changes in development mode
- Worker runs in continuous polling mode (5 second intervals)
- All changes committed to GitHub after each successful test phase
